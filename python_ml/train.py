#!/usr/bin/env python3
"""
Train a buyer-match model and export artifacts for inference.

Training data source:
- Prefer CSV at `python_ml/data/training_data.csv` (generated by `python_ml/generate_csv.py`)
- Fallback: synthetic rows generated in this file (legacy)

Goal:
- deterministic training (seeded)
- export artifacts used by infer.py (runtime inference)

Artifacts:
- artifacts/model.joblib (sklearn pipeline)
- artifacts/metadata.json (feature order + versions)

Note:
- This script prefers sklearn+pandas if installed. If not, it falls back to the legacy pure-python trainer.
"""

from __future__ import annotations

import json
import math
import os
import random
import csv
from dataclasses import dataclass
from datetime import date
from typing import Dict, List, Tuple


MODEL_VERSION = str(date.today())


def sigmoid(z: float) -> float:
    # numerically stable-ish sigmoid
    if z >= 0:
        ez = math.exp(-z)
        return 1.0 / (1.0 + ez)
    else:
        ez = math.exp(z)
        return ez / (1.0 + ez)


def dot(w: List[float], x: List[float]) -> float:
    return sum(wi * xi for wi, xi in zip(w, x))


@dataclass
class TrainingRow:
    x: List[float]
    y: int  # 0/1


FEATURE_NAMES = [
    "sectorMatch",
    "geoMatch",
    "sizeFit",
    "dryPowderFit",
    "activityLevel",
    "ebitdaFit",
]

SKLEARN_FEATURES = FEATURE_NAMES[:]  # explicit for metadata


def train_logistic_regression(
    data: List[TrainingRow],
    learning_rate: float = 0.12,
    epochs: int = 450,
) -> Tuple[List[float], float]:
    """
    Returns (weights, bias)
    """
    if not data:
        raise ValueError("No training data")

    dim = len(data[0].x)
    w = [0.0] * dim
    b = 0.0

    for _ in range(epochs):
        for row in data:
            z = dot(w, row.x) + b
            p = sigmoid(z)
            err = p - float(row.y)
            # SGD update
            for j in range(dim):
                w[j] -= learning_rate * err * row.x[j]
            b -= learning_rate * err * 1.0

    return w, b


def clamp01(x: float) -> float:
    if x != x or x == float("inf") or x == float("-inf"):
        return 0.0
    if x < 0:
        return 0.0
    if x > 1:
        return 1.0
    return x


def build_synthetic_rows(seed: int = 7) -> List[TrainingRow]:
    """
    Creates a synthetic dataset that encodes mandate-fit intuition:
    - sectorMatch/geoMatch/sizeFit/ebitdaFit matter most
    - dryPowderFit/activityLevel provide gradation
    """
    random.seed(seed)
    rows: List[TrainingRow] = []

    # Hand-anchored points
    anchors = [
        ([1, 1, 1, 1.0, 1.0, 1], 1),
        ([1, 1, 1, 0.7, 0.7, 1], 1),
        ([1, 1, 0, 0.8, 0.6, 1], 0),
        ([1, 0, 1, 0.8, 0.6, 1], 1),
        ([0, 1, 1, 0.9, 0.5, 1], 1),
        ([0, 0, 1, 0.9, 0.6, 1], 0),
        ([1, 1, 1, 0.2, 0.2, 1], 0),
        ([1, 1, 1, 0.9, 0.2, 0], 0),
        ([0, 0, 0, 0.1, 0.1, 0], 0),
    ]
    rows.extend(TrainingRow(x=x, y=y) for x, y in anchors)

    # Randomized samples with rule-based labeling
    for _ in range(650):
        sector = 1 if random.random() < 0.55 else 0
        geo = 1 if random.random() < 0.65 else 0
        size = 1 if random.random() < 0.6 else 0
        ebitda_fit = 1 if random.random() < 0.55 else 0
        dry = clamp01(random.random() ** 0.6)  # skew higher
        activity = clamp01(random.random() ** 0.7)

        x = [sector, geo, size, dry, activity, ebitda_fit]

        # Label heuristic:
        # - must have at least 3 of the 4 hard mandate bits (sector, geo, size, ebitda)
        hard = sector + geo + size + ebitda_fit
        if hard >= 3 and dry >= 0.35:
            y = 1
        elif hard == 2 and dry >= 0.75 and activity >= 0.55:
            y = 1
        else:
            y = 0

        # slight noise
        if random.random() < 0.03:
            y = 1 - y

        rows.append(TrainingRow(x=[float(v) for v in x], y=y))

    return rows


def load_rows_from_csv(csv_path: str) -> List[TrainingRow]:
    rows: List[TrainingRow] = []
    with open(csv_path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for r in reader:
            x = [
                float(r.get("sectorMatch", 0) or 0),
                float(r.get("geoMatch", 0) or 0),
                float(r.get("sizeFit", 0) or 0),
                float(r.get("dryPowderFit", 0) or 0),
                float(r.get("activityLevel", 0) or 0),
                float(r.get("ebitdaFit", 0) or 0),
            ]
            y = int(float(r.get("label", 0) or 0))
            rows.append(TrainingRow(x=x, y=y))
    if not rows:
        raise ValueError(f"No rows loaded from {csv_path}")
    return rows


def export_model(path: str, weights: List[float], bias: float) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    payload: Dict[str, object] = {
        "modelType": "logistic_regression_sgd",
        "modelVersion": MODEL_VERSION,
        "featureNames": FEATURE_NAMES,
        "weights": weights,
        "bias": bias,
        "notes": "Dependency-free logistic regression trained on synthetic mandate-fit labels.",
    }
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)

def export_metadata(path: str, model_version: str, feature_names: List[str], extra: Dict[str, object]) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    payload: Dict[str, object] = {
        "modelVersion": model_version,
        "featureNames": feature_names,
        **extra,
    }
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)


def main() -> None:
    seed = int(os.environ.get("TRAIN_SEED", "7"))
    random.seed(seed)

    csv_path = os.path.join(os.path.dirname(__file__), "data", "training_data.csv")
    src = "csv" if os.path.exists(csv_path) else "synthetic_fallback"

    # Preferred: sklearn pipeline (opt-in to avoid accidental env issues)
    use_sklearn = (os.environ.get("TRAIN_USE_SKLEARN", "0") or "").strip() in ("1", "true", "True")
    try:
        if not use_sklearn:
            raise RuntimeError("TRAIN_USE_SKLEARN not enabled")

        import pandas as pd  # type: ignore
        from joblib import dump  # type: ignore
        from sklearn.calibration import CalibratedClassifierCV  # type: ignore
        from sklearn.linear_model import LogisticRegression  # type: ignore
        from sklearn.metrics import average_precision_score, roc_auc_score  # type: ignore
        from sklearn.model_selection import train_test_split  # type: ignore

        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"Missing {csv_path}. Run: python3 python_ml/generate_csv.py")

        df = pd.read_csv(csv_path)
        X = df[SKLEARN_FEATURES].astype(float)
        y = df["label"].astype(int)

        X_train, X_tmp, y_train, y_tmp = train_test_split(
            X, y, test_size=0.3, random_state=seed, stratify=y
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_tmp, y_tmp, test_size=0.5, random_state=seed, stratify=y_tmp
        )

        base = LogisticRegression(max_iter=2000, solver="lbfgs")
        clf = CalibratedClassifierCV(base, method="isotonic", cv=3)
        clf.fit(X_train, y_train)

        # metrics
        def probs(m, X_):
            return m.predict_proba(X_)[:, 1]

        p_val = probs(clf, X_val)
        p_test = probs(clf, X_test)
        val_auc = float(roc_auc_score(y_val, p_val))
        test_auc = float(roc_auc_score(y_test, p_test))
        val_ap = float(average_precision_score(y_val, p_val))
        test_ap = float(average_precision_score(y_test, p_test))

        out_dir = os.path.join(os.path.dirname(__file__), "artifacts")
        os.makedirs(out_dir, exist_ok=True)
        model_path = os.path.join(out_dir, "model.joblib")
        dump(clf, model_path)

        metadata_path = os.path.join(out_dir, "metadata.json")
        export_metadata(
            metadata_path,
            MODEL_VERSION,
            SKLEARN_FEATURES,
            {
                "source": src,
                "rows": int(len(df)),
                "seed": seed,
                "metrics": {
                    "val_auc": val_auc,
                    "test_auc": test_auc,
                    "val_ap": val_ap,
                    "test_ap": test_ap,
                },
            },
        )

        print(f"Wrote {model_path} + {metadata_path} (v={MODEL_VERSION}) source={src}")
        print(f"Metrics: val_auc={val_auc:.4f} test_auc={test_auc:.4f} val_ap={val_ap:.4f} test_ap={test_ap:.4f}")
        return

    except Exception as e:
        # Fallback: legacy pure-python weights
        rows = load_rows_from_csv(csv_path) if os.path.exists(csv_path) else build_synthetic_rows(seed=seed)
        w, b = train_logistic_regression(rows)
        out_path = os.path.join(os.path.dirname(__file__), "artifacts", "model.json")
        export_model(out_path, w, b)
        export_metadata(
            os.path.join(os.path.dirname(__file__), "artifacts", "metadata.json"),
            MODEL_VERSION,
            FEATURE_NAMES,
            {"source": src, "rows": int(len(rows)), "seed": seed, "trainer": "legacy_pure_python", "note": str(e)},
        )
        print(f"Wrote {out_path} (v={MODEL_VERSION}) source={src} rows={len(rows)} (legacy fallback)")


if __name__ == "__main__":
    main()


